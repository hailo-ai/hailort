{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Speech2Text Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the Speech2Text Python API for running Speech-to-Text models on Hailo hardware.\n",
    "\n",
    "The Speech2Text API provides audio transcription capabilities using Whisper-based models, supporting both transcription and translation tasks with configurable language settings.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Audio transcription with timestamped segments\n",
    "- Language translation capabilities\n",
    "- Support for multiple output formats (segments vs complete text)\n",
    "- Configurable task types (transcribe/translate)\n",
    "- Language-specific processing\n",
    "\n",
    "**Best Practice: Context Manager**\n",
    "This tutorial does not use context-manager to share resources between different cells. Make sure to create VDevice and Speech2Text using 'with' statements whenever possible. When not using 'with', use VDevice.release() and Speech2Text.release() to clean up resources.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Run the notebook inside the Python virtual environment: ```source hailo_virtualenv/bin/activate```\n",
    "* A Speech2Text HEF file (Hailo Executable Format for Speech-to-Text models)\n",
    "* Audio files in PCM float32 format (normalized to [-1.0, 1.0), mono, little-endian, 16 kHz)\n",
    "* NumPy for audio processing: ```pip install numpy```\n",
    "\n",
    "**Audio Format Requirements:**\n",
    "\n",
    "The audio input must be in a specific format for proper processing:\n",
    "\n",
    "- **Format**: PCM float32 normalized to [-1.0, 1.0)\n",
    "- **Channels**: Mono (single channel)\n",
    "- **Endianness**: Little-endian\n",
    "- **Sample Rate**: 16 kHz\n",
    "\n",
    "**Tutorial Structure:**\n",
    "\n",
    "* Basic Speech2Text initialization and audio loading\n",
    "* Transcription with timestamped segments\n",
    "* Complete text transcription\n",
    "* Language translation capabilities\n",
    "* Task configuration (transcribe vs translate)\n",
    "* Language-specific processing\n",
    "\n",
    "When inside the ```virtualenv```, use the command ``jupyter-notebook <tutorial-dir>`` to open a Jupyter server that contains the tutorials (default folder on GitHub: ``hailort/libhailort/bindings/python/platform/hailo_tutorials/notebooks/``).\n"
   ],
   "id": "09b36d7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech2Text Tutorial: Setup and Configuration\n",
    "\n",
    "from hailo_platform import VDevice\n",
    "from hailo_platform.genai import Speech2Text, Speech2TextTask\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuration - Update these paths for your setup\n",
    "MODEL_PATH = \"/your/hef/path/speech2text.hef\"  # Update this path\n",
    "AUDIO_FILE_PATH = \"/your/audio/file/path/audio.bin\"  # Update this path\n",
    "\n",
    "print(\"Model path: {}\".format(MODEL_PATH))\n",
    "print(\"Audio file path: {}\".format(AUDIO_FILE_PATH))\n",
    "\n",
    "vdevice = VDevice()\n",
    "print(\"Initializing Speech2Text... this may take a moment...\")\n",
    "speech2text = Speech2Text(vdevice, MODEL_PATH)\n",
    "print(\"Speech2Text initialized successfully!\")\n"
   ],
   "id": "50f09eea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Loading and Format Validation\n",
    "\n",
    "Load and validate audio data in the required format.\n"
   ],
   "id": "2411897d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(file_path):\n",
    "    \"\"\"\n",
    "    Load audio file as binary data, and convert it into numpy array.\n",
    "    Expected format: PCM float32 normalized to [-1.0, 1.0), mono, little-endian, 16 kHz\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        audio_data = f.read()\n",
    "    \n",
    "    print(f\"Loaded audio file: {file_path}\")\n",
    "    print(f\"Audio data size: {len(audio_data)} bytes\")\n",
    "    \n",
    "    audio_array = np.frombuffer(audio_data, dtype='<f4').copy()\n",
    "    return audio_array\n",
    "\n",
    "# Load audio file\n",
    "audio_data = load_audio_file(AUDIO_FILE_PATH)\n"
   ],
   "id": "69e08ca5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transcription with Segments\n",
    "\n",
    "Generate transcription with timestamped segments for detailed analysis.\n"
   ],
   "id": "14be2d85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Speech2TextTask.TRANSCRIBE  # or \"TRANSLATE\"\n",
    "language = \"en\"  # ISO-639-1 language code\n",
    "\n",
    "print(\"Generating transcription with segments...\")\n",
    "segments = speech2text.generate_all_segments(audio_data, task=task, language=language)\n",
    "\n",
    "print(\"\\nTranscription Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f\"Segment {i+1}: {segment}\")\n",
    "    print(\"-\" * 30)\n"
   ],
   "id": "7c2f326b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Text Transcription\n",
    "\n",
    "Generate complete transcription as a single string without timestamps.\n"
   ],
   "id": "47992e39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complete transcription\n",
    "print(\"Generating complete transcription...\")\n",
    "complete_text = speech2text.generate_all_text(audio_data, task=task, language=language)\n",
    "\n",
    "print(\"\\nComplete Transcription:\")\n",
    "print(\"=\" * 50)\n",
    "print(complete_text)\n"
   ],
   "id": "20758c1f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation Example\n",
    "\n",
    "Demonstrate translation capabilities by transcribing speech in one language and translating to another.\n"
   ],
   "id": "76fb300f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Speech2TextTask.TRANSLATE  # Translate to English\n",
    "language = \"es\"  # Source language\n",
    "\n",
    "print(\"Generating translation...\")\n",
    "translation_segments = speech2text.generate_all_segments(audio_data, task=task, language=language)\n",
    "\n",
    "print(\"\\nTranslation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i, segment in enumerate(translation_segments):\n",
    "    print(f\"Segment {i+1}: [{segment.start_sec:.2f}s - {segment.end_sec:.2f}s]\")\n",
    "    print(f\"Translation: {segment.text}\")\n",
    "    print(\"-\" * 30)\n"
   ],
   "id": "d3132caf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection Example\n",
    "\n",
    "Demonstrates automatic language detection.\n",
    "Instead of specifying a language, the model detects it from the audio input,\n",
    "allowing seamless transcription or translation of audio in an unknown language.\n"
   ],
   "id": "184c14d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect language by omitting the 'language' parameter\n",
    "print(\"Generating transcription with automatic language detection...\")\n",
    "auto_detected_segments = speech2text.generate_all_segments(audio_data, task=Speech2TextTask.TRANSCRIBE)\n",
    "\n",
    "print(\"\\nAuto-Detected Language Transcription Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i, segment in enumerate(auto_detected_segments):\n",
    "    print(f\"Segment {i+1}: [{segment.start_sec:.2f}s - {segment.end_sec:.2f}s]\")\n",
    "    print(f\"Text: {segment.text}\")\n",
    "    print(\"-\" * 30)\n"
   ],
   "id": "977d0b85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Example\n",
    "The GenAI HEF comes with tokenization information, allowing the encoding of text into tokens."
   ],
   "id": "49f3a984"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = speech2text.tokenize(complete_text)\n",
    "print(\"The transcription has {} tokens: {}\".format(len(tokens), tokens))"
   ],
   "id": "de9faad7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Resource Management\n",
    "\n",
    "Properly clean up resources when done (best practice: use context managers when possible)\n"
   ],
   "id": "395c1bad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"Cleaning up resources...\")\n",
    "speech2text.release()\n",
    "vdevice.release()\n",
    "print(\"Resources cleaned up successfully!\")\n"
   ],
   "id": "dfcea47d"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
