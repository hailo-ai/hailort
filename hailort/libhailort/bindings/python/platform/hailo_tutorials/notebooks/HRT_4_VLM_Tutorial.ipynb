{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python VLM (Vision Language Model) Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the VLM Python API for running Vision Language Models on Hailo hardware.\n",
    "\n",
    "The VLM API provides multi-modal capabilities, combining text and image inputs for comprehensive AI interactions. It supports both streaming and non-streaming text generation, context management, and various generation parameters.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Multi-modal input processing (text + images)\n",
    "- Streaming and non-streaming text generation\n",
    "- Support for multiple images per prompt\n",
    "- Automatic frame validation and conversion\n",
    "- Context management for multi-turn conversations\n",
    "\n",
    "**Best Practice: Structured Prompts**\n",
    "This tutorial uses **structured prompts** (list of JSON messages) exclusively. Structured prompts provide better control, consistency, and leverage the model's chat template effectively.\n",
    "\n",
    "**Best Practice: Context Manager**\n",
    "This tutorial does not use context-manager to share resources between different cells. Make sure to create VDevice and VLM using 'with' statements whenever possible. When not using 'with', use VDevice.release() and VLM.release() to clean up resources.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Run the notebook inside the Python virtual environment: ```source hailo_virtualenv/bin/activate```\n",
    "* A VLM HEF file (Hailo Executable Format for Vision Language Models)\n",
    "* Image files for testing (JPEG/PNG format)\n",
    "* OpenCV for image processing: ```pip install opencv-python```\n",
    "\n",
    "**Memory Optimization (Optional):**\n",
    "\n",
    "* For large models that may exceed device memory, enable client-side tokenization\n",
    "* Requires libhailort to be compiled with `HAILO_BUILD_CLIENT_TOKENIZER=ON`\n",
    "* Requires Rust toolchain (cargo, rustup) to be installed on the build machine\n",
    "* Set `OPTIMIZE_MEMORY_ON_DEVICE = True` in the configuration section below\n",
    "\n",
    "**Tutorial Structure:**\n",
    "\n",
    "* Basic VLM initialization and image frame requirements\n",
    "* Image conversion using OpenCV (JPEG/PNG to numpy arrays)\n",
    "* Text-only generation (VLM without frames)\n",
    "* Single image processing with structured prompts\n",
    "* Multiple image processing\n",
    "* Generation parameters and context management\n",
    "* Advanced features: templates, tokenization, stop tokens\n",
    "\n",
    "When inside the ```virtualenv```, use the command ``jupyter-notebook <tutorial-dir>`` to open a Jupyter server that contains the tutorials (default folder on GitHub: ``hailort/libhailort/bindings/python/platform/hailo_tutorials/notebooks/``).\n"
   ],
   "id": "83ceef7e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM Tutorial: Setup and Configuration\n"
   ],
   "id": "aa8a7462"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from hailo_platform import VDevice\n",
    "from hailo_platform.genai import VLM\n",
    "\n",
    "# Configuration - Update these paths for your setup\n",
    "MODEL_PATH = \"/your/hef/path/vlm.hef\"  # Update this path\n",
    "SAMPLE_IMAGE_PATH = \"/path/to/your/image.jpg\"  # Update this path\n",
    "# Memory Optimization: Enable client-side tokenization for large models\n",
    "# This reduces device memory usage by moving tokenization to the host\n",
    "# Requires libhailort to be compiled with HAILO_BUILD_CLIENT_TOKENIZER=ON\n",
    "OPTIMIZE_MEMORY_ON_DEVICE = False  # Set to True for memory optimization\n",
    "\n",
    "print(\"Model path: {}\".format(MODEL_PATH))\n",
    "print(\"Sample image: {}\".format(SAMPLE_IMAGE_PATH))\n",
    "vdevice = VDevice()\n",
    "print(\"Initializing VLM... this may take a moment...\")\n",
    "vlm = VLM(vdevice, MODEL_PATH, OPTIMIZE_MEMORY_ON_DEVICE)\n",
    "print(\"VLM initialized successfully!\")\n"
   ],
   "id": "92035232"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding VLM Frame Requirements\n"
   ],
   "id": "4a12ce87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frame requirements from the model\n",
    "frame_shape = vlm.input_frame_shape()\n",
    "frame_dtype = vlm.input_frame_format_type()\n",
    "frame_size = vlm.input_frame_size()\n",
    "frame_order = vlm.input_frame_format_order()\n",
    "\n",
    "print(\"Frame requirements:\")\n",
    "print(\"  Shape: {} (height, width, channels)\".format(frame_shape))\n",
    "print(\"  Data type: {}\".format(frame_dtype))\n",
    "print(\"  Size in bytes: {}\".format(frame_size))\n",
    "print(\"  Format order: {}\".format(frame_order))\n",
    "\n",
    "height, width, channels = frame_shape\n",
    "print(\"  Expected frame format: {}x{}x{} {}\".format(height, width, channels, frame_dtype))\n"
   ],
   "id": "b7ed1e64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Conversion Functions Using OpenCV\n"
   ],
   "id": "71fa6f2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_numpy(image_path, target_shape, target_dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Convert JPEG/PNG image to numpy array with required shape and dtype.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        target_shape (tuple): Target shape (height, width, channels)\n",
    "        target_dtype: Target numpy data type\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Converted image array\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(\"Image file not found: {}\".format(image_path))\n",
    "    \n",
    "    target_height, target_width, target_channels = target_shape\n",
    "    \n",
    "    # Read image using OpenCV\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Failed to load image: {}\".format(image_path))\n",
    "    \n",
    "    # Convert BGR to RGB (OpenCV uses BGR by default)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if img.shape[0] != target_height or img.shape[1] != target_width:\n",
    "        img = cv2.resize(img, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n",
    "        print(\"Image resized to: {}x{}\".format(target_width, target_height))\n",
    "    \n",
    "    # Handle channel conversion if needed\n",
    "    if target_channels == 1 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = np.expand_dims(img, axis=2)\n",
    "        print(\"Converted to grayscale\")\n",
    "    elif target_channels == 3 and len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        print(\"Converted to RGB\")\n",
    "    \n",
    "    # Convert to target data type\n",
    "    img = img.astype(target_dtype)\n",
    "    \n",
    "    print(\"Final image shape: {}, dtype: {}\".format(img.shape, img.dtype))\n",
    "    return img\n",
    "\n",
    "def create_sample_image(shape, dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Create a sample image for testing when no image file is available.\n",
    "    \n",
    "    Args:\n",
    "        shape (tuple): Image shape (height, width, channels)\n",
    "        dtype: Data type for the image\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Generated sample image\n",
    "    \"\"\"\n",
    "    height, width, channels = shape\n",
    "    \n",
    "    # Create a gradient pattern\n",
    "    img = np.zeros(shape, dtype=dtype)\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if channels == 3:\n",
    "                img[i, j, 0] = int((i / height) * 255)  # Red gradient\n",
    "                img[i, j, 1] = int((j / width) * 255)   # Green gradient\n",
    "                img[i, j, 2] = 128                      # Blue constant\n",
    "            else:\n",
    "                img[i, j] = int(((i + j) / (height + width)) * 255)\n",
    "    \n",
    "    print(\"Created sample image with shape: {}, dtype: {}\".format(img.shape, img.dtype))\n",
    "    return img\n",
    "\n",
    "# Test the conversion function\n",
    "print(\"Testing image conversion functions...\")\n",
    "if os.path.exists(SAMPLE_IMAGE_PATH):\n",
    "    test_frame = convert_image_to_numpy(SAMPLE_IMAGE_PATH, frame_shape, frame_dtype)\n",
    "else:\n",
    "    print(\"Sample image not found, creating synthetic image...\")\n",
    "    test_frame = create_sample_image(frame_shape, frame_dtype)\n",
    "    \n",
    "print(\"Test frame ready for VLM processing!\")\n"
   ],
   "id": "c9abb1e3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Processing with Streaming Generation\n",
    "\n",
    "**Important:** The number of images in the 'frames' list must match the number of image entries in the structured prompt content.\n"
   ],
   "id": "c1f5fd41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image structured prompt\n",
    "single_image_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},  # One image placeholder\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# Streaming generation with one image\n",
    "with vlm.generate(prompt=single_image_prompt, frames=[test_frame], max_generated_tokens=40) as generation:\n",
    "    for token in generation:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n"
   ],
   "id": "3303d117"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Processing with Non-Streaming Generation\n"
   ],
   "id": "dc05b4af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear context for fresh conversation\n",
    "vlm.clear_context()\n",
    "\n",
    "# Non-streaming generation with one image\n",
    "analysis_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What colors do you see in this image? List the main objects.\"}, \n",
    "        {\"type\": \"image\"}  # One image placeholder\n",
    "    ]}\n",
    "]\n",
    "\n",
    "print(vlm.generate_all(prompt=analysis_prompt, frames=[test_frame], max_generated_tokens=40))\n"
   ],
   "id": "5acf3f07"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Only Generation (VLM without Images)\n",
    "\n",
    "VLM can work without any images by providing an empty frames list. In this case, it behaves like a regular LLM.\n"
   ],
   "id": "32f7ddad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-only structured prompt (no images)\n",
    "text_only_prompt = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful AI assistant.\"}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Explain what a vision language model is in 2 sentences.\"}]}\n",
    "]\n",
    "\n",
    "# Generate with empty frames list\n",
    "print(vlm.generate_all(prompt=text_only_prompt, frames=[], max_generated_tokens=50))\n"
   ],
   "id": "63eab99e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Image Processing\n",
    "\n",
    "When using multiple images, ensure the frames list contains exactly the same number of images as image placeholders in the prompt.\n"
   ],
   "id": "e745be49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second test image (different pattern)\n",
    "second_test_frame = create_sample_image(frame_shape, frame_dtype)\n",
    "# Modify the second image to make it different\n",
    "second_test_frame = second_test_frame * 0.7  # Darker version\n",
    "second_test_frame = second_test_frame.astype(frame_dtype)\n",
    "\n",
    "# Multiple images structured prompt\n",
    "multi_image_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},  # First image placeholder\n",
    "        {\"type\": \"image\"},  # Second image placeholder  \n",
    "        {\"type\": \"text\", \"text\": \"Compare these two images. What are the main differences?\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# Clear context for fresh conversation\n",
    "vlm.clear_context()\n",
    "\n",
    "# Generate with two images - IMPORTANT: frames list must match image count in prompt\n",
    "with vlm.generate(prompt=multi_image_prompt,\n",
    "    frames=[test_frame, second_test_frame],  # Two frames for two image placeholders\n",
    "    max_generated_tokens=100) as gen:\n",
    "    print(\"\".join(gen))\n"
   ],
   "id": "f1677a4e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn Conversations with Images\n"
   ],
   "id": "7366330c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear context for fresh conversation\n",
    "vlm.clear_context()\n",
    "\n",
    "# Turn 1: Initial image analysis\n",
    "turn1_prompt = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are an expert image analyst.\"}]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"}, \n",
    "        {\"type\": \"text\", \"text\": \"What's the dominant color in this image?\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "with vlm.generate(prompt=turn1_prompt, frames=[test_frame], max_generated_tokens=50) as gen:\n",
    "    print(\"\".join(gen))\n",
    "\n",
    "# Turn 2: Follow-up question (context maintained automatically)\n",
    "turn2_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Can you suggest what type of scene this might be?\"}]}\n",
    "]\n",
    "\n",
    "print(vlm.generate_all(prompt=turn2_prompt, frames=[], max_generated_tokens=50))\n"
   ],
   "id": "e24fc726"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Context Management\n",
    "\n",
    "Context is maintained between generate() calls automatically.\n",
    "The VLM API provides context management capabilities for monitoring and controlling conversation context, including image context. This is essential for:\n",
    "\n",
    "- **Memory Management**: Monitor context usage to avoid exceeding `max_context_capacity`, which can affect model accuracy\n",
    "- **Conversation Persistence**: Save and restore conversation states across sessions (including image context)\n",
    "- **Multi-Session Applications**: Maintain separate contexts for different users or topics\n",
    "\n",
    "**Key Functions:**\n",
    "\n",
    "- `get_context_usage_size()`: Returns current number of tokens in context\n",
    "- `max_context_capacity()`: Returns maximum context capacity of the model\n",
    "- `clear_context()`: Resets context usage to 0 and clears conversation history\n",
    "- `save_context()`: Saves current context as binary data (including image context)\n",
    "- `load_context(context_data)`: Loads a previously saved context\n"
   ],
   "id": "e64191d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Management Example\n",
    "\n",
    "# Check initial state\n",
    "vlm.clear_context()\n",
    "print(\"Max context capacity: {} tokens\".format(vlm.max_context_capacity()))\n",
    "print(\"Initial context usage: {} tokens\".format(vlm.get_context_usage_size()))\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"}, \n",
    "        {\"type\": \"text\", \"text\": \"What do you see in this image?\"}\n",
    "    ]}\n",
    "]\n",
    "response = vlm.generate_all(conversation, frames=[test_frame], max_generated_tokens=30)\n",
    "print(\"Response:\", response)\n",
    "print(\"Context usage after generation: {} tokens\".format(vlm.get_context_usage_size()))\n",
    "\n",
    "# Save context (including image context)\n",
    "saved_context = vlm.save_context()\n",
    "print(\"Context saved ({} bytes)\".format(len(saved_context)))\n",
    "\n",
    "# Clear context and demonstrate it's empty\n",
    "vlm.clear_context()\n",
    "print(\"After clear_context(): {} tokens\".format(vlm.get_context_usage_size()))\n",
    "\n",
    "# Load saved context and continue conversation\n",
    "vlm.load_context(saved_context)\n",
    "print(\"After load_context(): {} tokens\".format(vlm.get_context_usage_size()))\n",
    "continuation = [{\"role\": \"user\", \"content\": \"Describe the colors.\"}]\n",
    "print(\"Continuation:\", vlm.generate_all(continuation, frames=[], max_generated_tokens=20))\n"
   ],
   "id": "74a76629"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Parameters Example\n",
    "Temperature - used to control the model's creativity.\n",
    "More configurable parameters can be found in the API documentation.\n",
    "\n"
   ],
   "id": "0401f7cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"}, \n",
    "        {\"type\": \"text\", \"text\": \"Describe this image creatively.\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# Test different temperature settings\n",
    "\n",
    "# Low temperature (more deterministic)\n",
    "vlm.clear_context()\n",
    "response_low = vlm.generate_all(\n",
    "    prompt=test_prompt, \n",
    "    frames=[test_frame], \n",
    "    temperature=0.1, \n",
    "    seed=42, \n",
    "    max_generated_tokens=40\n",
    ")\n",
    "print(\"Low temperature (0.1): {}\".format(response_low))\n",
    "\n",
    "# High temperature (more creative)\n",
    "vlm.clear_context()\n",
    "response_high = vlm.generate_all(\n",
    "    prompt=test_prompt, \n",
    "    frames=[test_frame], \n",
    "    temperature=0.9, \n",
    "    seed=42, \n",
    "    max_generated_tokens=40\n",
    ")\n",
    "print(\"High temperature (0.9): {}\".format(response_high))\n"
   ],
   "id": "3a2bb69d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Prompts vs Structured Prompts Example\n",
    "\n",
    "VLM supports both structured prompts (recommended) and raw text prompts with special tokens.\n",
    "Here we demonstrate the tokens for QWEN family. Special tokens and prompts structures can be obtained using 'vlm.prompt_template()'\n"
   ],
   "id": "7b28897c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm.clear_context()\n",
    "\n",
    "# Raw prompt with vision tokens (model-specific format)\n",
    "raw_prompt = \"<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n<|im_start|>user\\\\nDescribe this image<|vision_start|><|image_pad|><|vision_end|><|im_end|>\\\\n<|im_start|>assistant\\\\n\"\n",
    "\n",
    "print(vlm.generate_all(prompt=raw_prompt, frames=[test_frame], max_generated_tokens=40, seed=100))\n",
    "print()\n",
    "\n",
    "vlm.clear_context()\n",
    "\n",
    "# Structured prompt (recommended)\n",
    "structured_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image\"}, \n",
    "        {\"type\": \"image\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "print(vlm.generate_all(prompt=structured_prompt, frames=[test_frame], max_generated_tokens=40, seed=100))\n",
    "print()\n",
    "\n",
    "print(\"Model prompt template:\")\n",
    "print(vlm.prompt_template())\n"
   ],
   "id": "c67ab06b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Example\n"
   ],
   "id": "834d1302"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"Describe this image\",\n",
    "    \"Vision language model with Hailo\",\n",
    "    \"What colors do you see in this image?\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization examples:\")\n",
    "for text in test_texts:\n",
    "    tokens = vlm.tokenize(text)\n",
    "    print(\"'{}' -> {} tokens: {}\".format(text, len(tokens), tokens))\n",
    "    \n",
    "print()\n"
   ],
   "id": "edccb68f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Tokens and Recovery Sequence Example\n"
   ],
   "id": "3d96ab33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current stop tokens\n",
    "original_stop_tokens = vlm.get_stop_tokens()\n",
    "print(\"Original stop tokens: {}\".format(original_stop_tokens))\n",
    "\n",
    "# Test with custom stop tokens\n",
    "custom_stop_tokens = [\".\", \"END\"]\n",
    "vlm.set_stop_tokens(custom_stop_tokens)\n",
    "print(\"Custom stop tokens: {}\".format(vlm.get_stop_tokens()))\n",
    "\n",
    "test_prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"}, \n",
    "        {\"type\": \"text\", \"text\": \"List three things you see. 1. Color patterns 2. Shapes 3. Textures.\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "vlm.clear_context()\n",
    "response = vlm.generate_all(prompt=test_prompt, frames=[test_frame], max_generated_tokens=50)\n",
    "print(\"Response with custom stop tokens: {}\".format(response))\n",
    "\n",
    "# Reset stop tokens\n",
    "vlm.set_stop_tokens(original_stop_tokens)\n",
    "print(\"Reset stop tokens: {}\".format(vlm.get_stop_tokens()))\n"
   ],
   "id": "4a51d462"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "**Key Points to Remember:**\n",
    "\n",
    "1. **Frame Count Matching**: Always ensure the number of frames matches the number of image placeholders in your structured prompt\n",
    "2. **Image Format**: Use `vlm.input_frame_shape()`, `vlm.input_frame_format_type()` to get correct format requirements\n",
    "3. **OpenCV Conversion**: Use BGRâ†’RGB conversion when loading images with OpenCV\n",
    "4. **Text-Only Mode**: VLM can work without images by using empty frames list `[]`\n",
    "5. **Context Management**: Use `vlm.clear_context()` to start fresh conversations. Use 'vlm.get_context_usage_size()' to see the number of tokens used in this context\n",
    "6. **Resource Cleanup**: Always call `vlm.release()` and `vdevice.release()` when done\n",
    "\n",
    "**Structured Prompt Format for VLM:**\n",
    "```python\n",
    "prompt = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Your text here\"},\n",
    "        {\"type\": \"image\"},  # One per image in frames list\n",
    "        {\"type\": \"text\", \"text\": \"More text if needed\"}\n",
    "    ]}\n",
    "]\n",
    "```\n"
   ],
   "id": "dce792ae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Resource Management\n",
    "\n",
    "Properly clean up resources when done (best practice: use context managers when possible)\n"
   ],
   "id": "0ce2e117"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm.release()\n",
    "vdevice.release()\n",
    "print(\"Resources released successfully\")\n",
    "print(\"VLM tutorial completed!\")\n"
   ],
   "id": "ffdc5f47"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
