{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python Inference Tutorial - Multi Process Service and Model Scheduler\n",
    "\n",
    "This tutorial describes how to run an inference process using `InferPipeline` API (sync API), which is an alternative to the recommended Async API with, multi-process service and the Model Scheduler\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Enable HailoRT Multi-Process Service before running inference. For instructions, see [Multi Process Service](https://hailo.ai/developer-zone/documentation/hailort/latest/?sp_referrer=inference/inference.html#multi-process-service).\n",
    "* Run the notebook inside the Python virtual environment: ```source hailo_virtualenv/bin/activate```\n",
    "\n",
    "When inside the ```virtualenv```, use the command ``hailo tutorial`` to open a Jupyter server that contains the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Process\n",
    "from hailo_platform import (HEF, VDevice, HailoStreamInterface, InferVStreams, ConfigureParams,\n",
    "    InputVStreamParams, OutputVStreamParams, InputVStreams, OutputVStreams, FormatType, HailoSchedulingAlgorithm)\n",
    "\n",
    "\n",
    "# Define the function to run inference on the model\n",
    "def infer(network_group, input_vstreams_params, output_vstreams_params, input_data):\n",
    "    rep_count = 100\n",
    "    with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:\n",
    "        for i in range(rep_count):\n",
    "            infer_results = infer_pipeline.infer(input_data)\n",
    "\n",
    "\n",
    "def create_vdevice_and_infer(hef_path):\n",
    "    # Creating the VDevice target with scheduler enabled\n",
    "    params = VDevice.create_params()\n",
    "    params.scheduling_algorithm = HailoSchedulingAlgorithm.ROUND_ROBIN\n",
    "    params.multi_process_service = True\n",
    "    params.group_id = \"SHARED\"\n",
    "    with VDevice(params) as target:\n",
    "        configure_params = ConfigureParams.create_from_hef(hef=hef, interface=HailoStreamInterface.PCIe)\n",
    "        model_name = hef.get_network_group_names()[0]\n",
    "        batch_size = 2\n",
    "        configure_params[model_name].batch_size = batch_size\n",
    "        \n",
    "        network_groups = target.configure(hef, configure_params)\n",
    "        network_group = network_groups[0]\n",
    "\n",
    "        # Create input and output virtual streams params\n",
    "        input_vstreams_params = InputVStreamParams.make(network_group, format_type=FormatType.FLOAT32)\n",
    "        output_vstreams_params = OutputVStreamParams.make(network_group, format_type=FormatType.UINT8)\n",
    "\n",
    "        # Define dataset params\n",
    "        input_vstream_info = hef.get_input_vstream_infos()[0]\n",
    "        image_height, image_width, channels = input_vstream_info.shape\n",
    "        num_of_frames = 10\n",
    "        low, high = 2, 20\n",
    "\n",
    "        # Generate random dataset\n",
    "        dataset = np.random.randint(low, high, (num_of_frames, image_height, image_width, channels)).astype(np.float32)\n",
    "        input_data = {input_vstream_info.name: dataset}\n",
    "\n",
    "        infer(network_group, input_vstreams_params, output_vstreams_params, input_data)\n",
    "\n",
    "# Loading compiled HEFs:\n",
    "first_hef_path = '../hefs/resnet_v1_18.hef'\n",
    "second_hef_path = '../hefs/shortcut_net.hef'\n",
    "first_hef = HEF(first_hef_path)\n",
    "second_hef = HEF(second_hef_path)\n",
    "hefs = [first_hef, second_hef]\n",
    "infer_processes = []\n",
    "\n",
    "# Configure network groups\n",
    "for hef in hefs:\n",
    "    # Create infer process\n",
    "    infer_process = Process(target=create_vdevice_and_infer, args=(hef,))\n",
    "    infer_processes.append(infer_process)\n",
    "\n",
    "print(f'Starting inference on multiple models using scheduler')\n",
    "\n",
    "infer_failed = False\n",
    "for infer_process in infer_processes:\n",
    "    infer_process.start()\n",
    "for infer_process in infer_processes:\n",
    "    infer_process.join()\n",
    "    if infer_process.exitcode:\n",
    "        infer_failed = True\n",
    "        \n",
    "if infer_failed:\n",
    "    raise Exception(\"infer process failed\")\n",
    "print('Done inference')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
