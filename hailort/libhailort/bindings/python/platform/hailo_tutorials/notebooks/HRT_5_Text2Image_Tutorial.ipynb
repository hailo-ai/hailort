{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python Text2Image (Stable Diffusion) Tutorial\n",
        "\n",
        "This tutorial demonstrates how to use the Text2Image Python API to generate images from text prompts on Hailo hardware.\n",
        "\n",
        "The Text2Image API provides direct generation as well as advanced controls over diffusion steps, guidance scale, seed, scheduler type and more.\n",
        "\n",
        "**Best Practice: context-manager**\n",
        "This tutorial doesnt use context-manager, to share resources between different cells. Make sure to create VDevice and Text2Image using 'with' statements whenever possible. When not using 'with', use VDevice.release() and Text2Image.release() to cleanup resources.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "* Run the notebook inside the Python virtual environment: ```source hailo_virtualenv/bin/activate```\n",
        "* Three HEF files: denoiser, text encoder, and image decoder\n",
        "\n",
        "**Tutorial Structure:**\n",
        "\n",
        "* Basic Text2Image initialization and simple generation\n",
        "* Generation parameters (steps_count, guidance_scale, samples_count, seed)\n",
        "* Output image metadata and tokenization\n",
        "* Optional: using a different diffusion scheduler (DDIM)\n",
        "\n",
        "When inside the ```virtualenv```, use the command ``jupyter-notebook <tutorial-dir>`` to open a Jupyter server that contains the tutorials (default folder on GitHub: ``hailort/libhailort/bindings/python/platform/hailo_tutorials/notebooks/``).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text2Image Tutorial: Setup and Configuration\n",
        "\n",
        "from hailo_platform import VDevice\n",
        "from hailo_platform.genai import Text2Image\n",
        "from hailo_platform.pyhailort.pyhailort import HailoDiffuserSchedulerType\n",
        "\n",
        "# Configuration - Update these paths for your setup\n",
        "DENOISE_HEF = \"/your/hef/path/denoise.hef\"           # Update this path\n",
        "TEXT_ENCODER_HEF = \"/your/hef/path/text_encoder.hef\"  # Update this path\n",
        "IMAGE_DECODER_HEF = \"/your/hef/path/image_decoder.hef\"# Update this path\n",
        "\n",
        "print(\"DENOISE_HEF: {}\".format(DENOISE_HEF))\n",
        "print(\"TEXT_ENCODER_HEF: {}\".format(TEXT_ENCODER_HEF))\n",
        "print(\"IMAGE_DECODER_HEF: {}\".format(IMAGE_DECODER_HEF))\n",
        "\n",
        "vdevice = VDevice()\n",
        "print(\"Initializing Text2Image... this may take a moment...\")\n",
        "t2i = Text2Image(vdevice, DENOISE_HEF, TEXT_ENCODER_HEF, IMAGE_DECODER_HEF)\n",
        "print(\"Text2Image initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Generation Example\n",
        "\n",
        "Generate a single image from a short prompt with default parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# It is best practice to use negative prompt to guide what should be avoided in the generated image\n",
        "NEGATIVE_PROMPT = \"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, \"\\\n",
        "    \"disfigured, deformed, body out of frame, bad anatomy, watermark, signature,cut off, low contrast, underexposed, \"\\\n",
        "    \"overexposed, bad art, beginner, amateur, distorted face\"\n",
        "\n",
        "images = t2i.generate(\n",
        "    prompt=\"A scenic mountain landscape at sunrise\",\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    samples_count=1,\n",
        "    steps_count=5,\n",
        ")\n",
        "print(\"Generated {} image(s)\".format(len(images)))\n",
        "img = images[0]\n",
        "print(\"Image shape: {} | dtype: {}\".format(img.shape, img.dtype))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation Parameters Example\n",
        "\n",
        "Tune quality, style adherence, reproducibility, and samples-count size using parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multiple samples with guidance and reproducible seed\n",
        "images_multi = t2i.generate(\n",
        "    prompt=\"A cat wearing sunglasses\",\n",
        "    negative_prompt=NEGATIVE_PROMPT,\n",
        "    samples_count=2,\n",
        "    steps_count=8,\n",
        "    guidance_scale=7.5,\n",
        ")\n",
        "print(\"Batch generated {} images\".format(len(images_multi)))\n",
        "\n",
        "# Seed reproducibility\n",
        "seed = 42\n",
        "img_a = t2i.generate(\"A red sports car\", samples_count=1, steps_count=6, seed=seed)[0]\n",
        "img_b = t2i.generate(\"A red sports car\", samples_count=1, steps_count=6, seed=seed)[0]\n",
        "print(\"Deterministic with same seed:\", (img_a == img_b).all())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Image Metadata & Tokenization\n",
        "\n",
        "Query output shape and type, and use built-in tokenizer for prompt preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output metadata\n",
        "shape = t2i.output_sample_shape()\n",
        "dtype = t2i.output_sample_format_type()\n",
        "order = t2i.output_sample_format_order()\n",
        "print(\"Expected output shape: {} | dtype: {} | order: {}\".format(shape, dtype, order))\n",
        "\n",
        "# Tokenization\n",
        "for text in [\"Hello world\", \"A beautiful sunset\"]:\n",
        "    tokens = t2i.tokenize(text)\n",
        "    print(\"'{}' -> {} tokens\".format(text, len(tokens)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Different Diffusion Scheduler (DDIM)\n",
        "\n",
        "Create `Text2Image` with a different scheduler to vary denoising behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-initialize with DDIM scheduler (optional)\n",
        "t2i.release()\n",
        "\n",
        "t2i = Text2Image(vdevice, DENOISE_HEF, TEXT_ENCODER_HEF, IMAGE_DECODER_HEF,\n",
        "                 scheduler_type=HailoDiffuserSchedulerType.DDIM)\n",
        "\n",
        "# Quick generation with DDIM\n",
        "img_ddim = t2i.generate(\"A watercolor painting of a city skyline\", negative_prompt=NEGATIVE_PROMPT, samples_count=1, steps_count=6)[0]\n",
        "print(\"Generated image with DDIM | shape: {} | dtype: {}\".format(img_ddim.shape, img_ddim.dtype))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up resources (best practice: use context managers when possible)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t2i.release()\n",
        "vdevice.release()\n",
        "print(\"Resources released successfully\")\n",
        "print(\"Text2Image tutorial completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
