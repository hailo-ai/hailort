{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Python LLM (Large Language Model) Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the LLM Python API for running Large Language Models on Hailo hardware.\n",
    "\n",
    "The LLM API provides both streaming and non-streaming text generation capabilities, context management, and various generation parameters for fine-tuned control over model outputs.\n",
    "\n",
    "**Best Practice: Structured Prompts**\n",
    "This tutorial uses **structured prompts** (list of JSON messages) exclusively. Structured prompts provide better control, consistency, and leverage the model's chat template effectively.\n",
    "\n",
    "**Best Practice: context-manager**\n",
    "This tutorial does not use context-manager, to share resources between different cells. Make sure to create VDevice and LLM using 'with' statements whenever possible. When not using 'with', use VDevice.release() and LLM.release() to clean up resources.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Run the notebook inside the Python virtual environment: ```source hailo_virtualenv/bin/activate```\n",
    "* An LLM HEF file (Hailo Executable Format for Large Language Models)\n",
    "* Optional: LoRA (Low-Rank Adaptation) name for fine-tuned models inside this HEF\n",
    "\n",
    "**Memory Optimization (Optional):**\n",
    "\n",
    "* For large models that may exceed device memory, enable client-side tokenization\n",
    "* Requires libhailort to be compiled with `HAILO_BUILD_CLIENT_TOKENIZER=ON`\n",
    "* Requires Rust toolchain (cargo, rustup) to be installed on the build machine\n",
    "* Set `OPTIMIZE_MEMORY_ON_DEVICE = True` in the configuration section below\n",
    "\n",
    "**Tutorial Structure:**\n",
    "\n",
    "* Basic LLM initialization and simple generation, Streaming vs non-streaming\n",
    "* Generation parameters (temperature, top_p, top_k, etc.)\n",
    "* Context management for multi-turn conversations\n",
    "* Advanced features: templates, tokenization, stop tokens\n",
    "\n",
    "When inside the ```virtualenv```, use the command ``jupyter-notebook <tutorial-dir>`` to open a Jupyter server that contains the tutorials (default folder on GitHub: ``hailort/libhailort/bindings/python/platform/hailo_tutorials/notebooks/``).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Tutorial: Setup and Configuration\n",
    "\n",
    "from hailo_platform import VDevice\n",
    "from hailo_platform.genai import LLM\n",
    "\n",
    "# Configuration - Update these paths for your setup\n",
    "MODEL_PATH = \"/your/hef/path/llm.hef\"  # Update this path\n",
    "LORA_NAME = \"\"  # Optional: specify LoRA adapter name\n",
    "\n",
    "print(\"Model path: {}\".format(MODEL_PATH))\n",
    "print(\"LoRA adapter: {}\".format(LORA_NAME if LORA_NAME else 'None'))\n",
    "vdevice = VDevice()\n",
    "print(\"Initializing LLM... this may take a moment...\")\n",
    "llm = LLM(vdevice, MODEL_PATH, LORA_NAME)\n",
    "print(\"LLM initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Streaming Generation Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured prompt (recommended approach)\n",
    "structured_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning in 2 sentences.\"}\n",
    "]\n",
    "\n",
    "with llm.generate(structured_prompt, max_generated_tokens=50, seed=31) as generation:\n",
    "    for token in generation:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Non-Streaming Generation Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.clear_context()\n",
    "print(llm.generate_all(structured_prompt, max_generated_tokens=50, seed=31))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Multi-Turn Conversations Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear context\n",
    "llm.clear_context()\n",
    "\n",
    "# Turn 1: Introduction\n",
    "conversation_1 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm learning Python. What's a list?\"}\n",
    "]\n",
    "\n",
    "print(llm.generate_all(conversation_1, max_generated_tokens=60))\n",
    "print()\n",
    "\n",
    "# Turn 2: Follow-up (context maintained automatically)\n",
    "followup = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you show me an example?\"}\n",
    "]\n",
    "\n",
    "print(llm.generate_all(followup, max_generated_tokens=60, seed=6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Context Management Example\n",
    "Context is maintained between generate() calls automatically.\n",
    "Use 'clear_context()' to start fresh conversations\n",
    "Use 'get_context_usage_size()' to see how many tokens are used in this conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your profession?\"}\n",
    "]\n",
    "\n",
    "print(llm.generate_all(question, max_generated_tokens=30))\n",
    "new_question = [\n",
    "    {\"role\": \"user\", \"content\": \"What were we just discussing?\"}\n",
    "]\n",
    "print(llm.generate_all(new_question, max_generated_tokens=30))\n",
    "print(\"Number of tokens in the context - {}\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Clear context and ask again\n",
    "llm.clear_context()\n",
    "print(\"Number of tokens in the context after 'llm.clear_context()' - {}\".format(llm.get_context_usage_size()))\n",
    "\n",
    "print(llm.generate_all(new_question, max_generated_tokens=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling / Tool Use Example\n",
    "\n",
    "**What are Tools?**\n",
    "\n",
    "Tools (also called \"function calling\") allow LLMs to interact with external functions and APIs. Instead of just generating text, the model can request to call specific functions with appropriate parameters to retrieve real-time information, perform calculations, or interact with external services.\n",
    "\n",
    "**How to Use Tools:**\n",
    "\n",
    "1. **Define Tools:** Create tool definitions in JSON schema format (following OpenAI's function calling schema). Each tool needs a name, description, and parameter specifications.\n",
    "\n",
    "2. **Provide Tools:** Pass the `tools` list to the first `generate()` or `generate_all()` call. Tools can only be provided on a **fresh context** (first call or after `clear_context()`).\n",
    "\n",
    "3. **Parse Model Response:** The model will output a tool call request (format depends on the model's chat template). Parse the tool name and arguments from the response.\n",
    "\n",
    "4. **Execute Function:** Run the actual function in your code (e.g., call a weather API) and get the result.\n",
    "\n",
    "5. **Feed Result Back:** Continue the conversation by providing the tool result in your next message. The model will use this result to generate its final response.\n",
    "\n",
    "**Important Notes:**\n",
    "- The model doesn't execute functions - it only requests tool calls\n",
    "- Tools are registered once per context and persist throughout the conversation\n",
    "- To change tools, call `clear_context()` first\n",
    "\n",
    "**Learn More:** [HuggingFace Tool Use Guide](https://huggingface.co/docs/transformers/en/chat_extras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear context to start fresh (required for providing tools)\n",
    "llm.clear_context()\n",
    "\n",
    "# Step 1: Define tools (JSON schema format)\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_temperature\",\n",
    "            \"description\": \"Get current temperature at a location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature_date\",\n",
    "            \"description\": \"Get temperature at a location and date.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The date to get the temperature for, in the format \\\"Year-Month-Day\\\".\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"date\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 2: First turn - User asks about current temperature, tools are provided\n",
    "user_question = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the current temperature in Paris, France?\"}\n",
    "]\n",
    "\n",
    "response = llm.generate_all(user_question, tools=tools, max_generated_tokens=60, timeout_ms=30000)\n",
    "print(\"User: What's the current temperature in Paris, France?\")\n",
    "print(\"\\nModel response:\")\n",
    "print(response)\n",
    "\n",
    "# Step 3: Simulate tool execution\n",
    "# Note: In a real application, you would:\n",
    "# 1. Parse the tool call from the model's response\n",
    "# 2. Execute the actual function (e.g. call a weather API - get_current_temperature(location='Paris, France', unit='celsius'))\n",
    "# 3. Get the real result (e.g. 18C)\n",
    "\n",
    "# Step 4: Feed the result back to the model\n",
    "# In a real scenario, the tool result would be properly formatted\n",
    "# Here we simulate the conversation continuing with the information\n",
    "follow_up = [\n",
    "    {\"role\": \"user\", \"content\": \"The current temperature is 18C.\"}\n",
    "]\n",
    "\n",
    "response = llm.generate_all(follow_up, max_generated_tokens=60, timeout_ms=30000)\n",
    "print(\"\\nModel response with tool result:\")\n",
    "print(response)\n",
    "\n",
    "# Step 5: Conversation can continue (tools persist throughout context)\n",
    "# Ask about historical temperature\n",
    "historical_question = [\n",
    "    {\"role\": \"user\", \"content\": \"What was the temperature in Paris on 2024-01-15?\"}\n",
    "]\n",
    "\n",
    "response = llm.generate_all(historical_question, max_generated_tokens=60, timeout_ms=30000)\n",
    "print(\"\\nUser: What was the temperature in Paris on 2024-01-15?\")\n",
    "print(\"Model response:\")\n",
    "print(response)\n",
    "\n",
    "# Note: The conversation continues without re-specifying tools.\n",
    "#       To use different tools, call llm.clear_context() first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Context Management\n",
    "\n",
    "The LLM API provides context management capabilities for monitoring and controlling conversation context. This is essential for:\n",
    "\n",
    "- **Memory Management**: Monitor context usage to avoid exceeding `max_context_capacity`, which can affect model accuracy\n",
    "- **Conversation Persistence**: Save and restore conversation states across sessions\n",
    "- **Multi-Session Applications**: Maintain separate contexts for different users or topics\n",
    "\n",
    "**Key Functions:**\n",
    "\n",
    "- `get_context_usage_size()`: Returns current number of tokens in context\n",
    "- `max_context_capacity()`: Returns maximum context capacity of the model  \n",
    "- `clear_context()`: Resets context usage to 0 and clears conversation history\n",
    "- `save_context()`: Saves current context as binary data\n",
    "- `load_context(context_data)`: Loads a previously saved context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Context Management Example\n",
    "\n",
    "# Check initial state\n",
    "llm.clear_context()\n",
    "print(\"Max context capacity: {} tokens\".format(llm.max_context_capacity()))\n",
    "print(\"Initial context usage: {} tokens\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Build conversation context\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning briefly.\"}\n",
    "]\n",
    "response = llm.generate_all(conversation, max_generated_tokens=30)\n",
    "print(\"Response:\", response)\n",
    "print(\"Context usage after generation: {} tokens\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Save context\n",
    "saved_context = llm.save_context()\n",
    "print(\"Context saved ({} bytes)\".format(len(saved_context)))\n",
    "\n",
    "# Clear context and demonstrate it's empty\n",
    "llm.clear_context()\n",
    "print(\"After clear_context(): {} tokens\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Load saved context and continue conversation\n",
    "llm.load_context(saved_context)\n",
    "print(\"After load_context(): {} tokens\".format(llm.get_context_usage_size()))\n",
    "continuation = [{\"role\": \"user\", \"content\": \"Give me an example.\"}]\n",
    "print(\"Continuation:\", llm.generate_all(continuation, max_generated_tokens=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Generation Parameters Example\n",
    "seed - used to ensure reproducible results.\n",
    "more configurable parameters can be found in the API documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about AI.\"}\n",
    "]\n",
    "# Reproducible with seed\n",
    "llm.clear_context()\n",
    "response_seed1 = llm.generate_all(test_prompt, seed=42, temperature=0.8, max_generated_tokens=25)\n",
    "llm.clear_context()\n",
    "response_seed2 = llm.generate_all(test_prompt, seed=42, temperature=0.8, max_generated_tokens=25)\n",
    "print(\"Seed=42 (run 1): {}\".format(response_seed1))\n",
    "print(\"Seed=42 (run 2): {}\".format(response_seed2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Raw Prompts vs Structured Prompts Example\n",
    "Raw prompt is a single string, usually wrapped with special tokens that are different per model.\n",
    "Here we demonstrates the tokens for QWEN family. Special tokens and prompt structures can be obtained using 'llm.prompt_template()'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.clear_context()\n",
    "raw_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat is machine learning?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "with llm.generate(raw_prompt, max_generated_tokens=30, seed=100) as generation:\n",
    "    print(\"\".join(generation))\n",
    "print()\n",
    "\n",
    "llm.clear_context()\n",
    "\n",
    "# Structured prompt (recommended)\n",
    "structured_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "]\n",
    "print(llm.generate_all(structured_prompt, max_generated_tokens=30, seed=100))\n",
    "print()\n",
    "print(llm.prompt_template())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Tokenization Example\n",
    "The GenAI HEF comes with tokenization information, allowing the encoding of text into tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"Hello world\",\n",
    "    \"Machine learning with Hailo\",\n",
    "    \"The quick brown fox jumps!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = llm.tokenize(text)\n",
    "    print(\"'{}' {} tokens: {}\".format(text, len(tokens), tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Stop Tokens and Recovery Sequence Example\n",
    "The generation will stop when one of these conditions is met:\n",
    "\n",
    "**Max Tokens Reached**\n",
    "\n",
    "* in this case, a custom recovery-sequence will be fed into the LLM\n",
    "* most models comes with default recovery sequence, which is retrievable and configurable using 'llm.get_generation_recovery_sequence()' and 'llm.set_generation_recovery_sequence()'.\n",
    "* the recovery-sequence tokens are not counted for 'max_generated_tokens'.\n",
    "\n",
    "**Logical End of Generation**\n",
    "\n",
    "* whenever hitting one of the stop_sequences, the model finishes its generation 'gracefully' (-> without any recovery-sequence).\n",
    "* a stop-token can be any sequence of tokens (str). the generation will stop when the exact string is generated as a sequence.\n",
    "* most models comes with default stop tokens, which are retrievable and configurable using 'llm.get_stop_tokens()' and 'llm.set_stop_tokens()'.\n",
    "* all stop tokens are checked after each generated token. Setting too many can affect performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current stop tokens\n",
    "original_stop_tokens = llm.get_stop_tokens()\n",
    "print(\"Original stop tokens: {}\".format(original_stop_tokens))\n",
    "\n",
    "# Set custom stop tokens\n",
    "custom_stop_tokens = [\".\", \"END\", \"\\\\n\"]\n",
    "llm.set_stop_tokens(custom_stop_tokens)\n",
    "print(\"Custom stop tokens: {}\".format(llm.get_stop_tokens()))\n",
    "print()\n",
    "\n",
    "# Set empty stop tokens - model will stop generation only when 'max_generated_tokens' is reached\n",
    "llm.set_stop_tokens([])\n",
    "test_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Count to 5: 1, 2, 3, 4, 5. Done.\"}\n",
    "]\n",
    "print(llm.generate_all(test_prompt, max_generated_tokens=10))\n",
    "print()\n",
    "\n",
    "# Reset stop tokens\n",
    "llm.set_stop_tokens(original_stop_tokens)\n",
    "print(\"Reset stop tokens: {}\".format(llm.get_stop_tokens()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Resource Management\n",
    "\n",
    "Properly clean up resources when done (best practice: use context managers when possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.release()\n",
    "vdevice.release()\n",
    "print(\"Resources released successfully\")\n",
    "print(\"LLM tutorial completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
