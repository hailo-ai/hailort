{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-necessity",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Python LLM (Large Language Model) Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the LLM Python API for running Large Language Models on Hailo hardware.\n",
    "\n",
    "The LLM API provides both streaming and non-streaming text generation capabilities, context management, and various generation parameters for fine-tuned control over model outputs.\n",
    "\n",
    "**Best Practice: Structured Prompts**\n",
    "This tutorial uses **structured prompts** (list of JSON messages) exclusively. Structured prompts provide better control, consistency, and leverage the model's chat template effectively.\n",
    "\n",
    "**Best Practice: context-manager**\n",
    "This tutorial does not use context-manager, to share resources between different cells. Make sure to create VDevice and LLM using 'with' statements whenever possible. When not using 'with', use VDevice.release() and LLM.release() to clean up resources.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* Run the notebook inside the Python virtual environment: ```source hailo_virtualenv/bin/activate```\n",
    "* An LLM HEF file (Hailo Executable Format for Large Language Models)\n",
    "* Optional: LoRA (Low-Rank Adaptation) name for fine-tuned models inside this HEF\n",
    "\n",
    "**Memory Optimization (Optional):**\n",
    "\n",
    "* For large models that may exceed device memory, enable client-side tokenization\n",
    "* Requires libhailort to be compiled with `HAILO_BUILD_CLIENT_TOKENIZER=ON`\n",
    "* Requires Rust toolchain (cargo, rustup) to be installed on the build machine\n",
    "* Set `OPTIMIZE_MEMORY_ON_DEVICE = True` in the configuration section below\n",
    "\n",
    "**Tutorial Structure:**\n",
    "\n",
    "* Basic LLM initialization and simple generation, Streaming vs non-streaming\n",
    "* Generation parameters (temperature, top_p, top_k, etc.)\n",
    "* Context management for multi-turn conversations\n",
    "* Advanced features: templates, tokenization, stop tokens\n",
    "\n",
    "When inside the ```virtualenv```, use the command ``jupyter-notebook <tutorial-dir>`` to open a Jupyter server that contains the tutorials (default folder on GitHub: ``hailort/libhailort/bindings/python/platform/hailo_tutorials/notebooks/``).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Tutorial: Setup and Configuration\n",
    "\n",
    "from hailo_platform import VDevice\n",
    "from hailo_platform.genai import LLM\n",
    "\n",
    "# Configuration - Update these paths for your setup\n",
    "MODEL_PATH = \"/your/hef/path/llm.hef\"  # Update this path\n",
    "LORA_NAME = \"\"  # Optional: specify LoRA adapter name\n",
    "\n",
    "print(\"Model path: {}\".format(MODEL_PATH))\n",
    "print(\"LoRA adapter: {}\".format(LORA_NAME if LORA_NAME else 'None'))\n",
    "vdevice = VDevice()\n",
    "print(\"Initializing LLM... this may take a moment...\")\n",
    "llm = LLM(vdevice, MODEL_PATH, LORA_NAME)\n",
    "print(\"LLM initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hello-goodbye",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Streaming Generation Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured prompt (recommended approach)\n",
    "structured_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning in 2 sentences.\"}\n",
    "]\n",
    "\n",
    "with llm.generate(structured_prompt, max_generated_tokens=50, seed=31) as generation:\n",
    "    for token in generation:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shalom-lehitraot",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Non-Streaming Generation Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.generate_all(structured_prompt, max_generated_tokens=50, seed=31))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-sky",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Multi-Turn Conversations Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear context\n",
    "llm.clear_context()\n",
    "\n",
    "# Turn 1: Introduction\n",
    "conversation_1 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm learning Python. What's a list?\"}\n",
    "]\n",
    "\n",
    "print(llm.generate_all(conversation_1, max_generated_tokens=60))\n",
    "print()\n",
    "\n",
    "# Turn 2: Follow-up (context maintained automatically)\n",
    "followup = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you show me an example?\"}\n",
    "]\n",
    "\n",
    "print(llm.generate_all(followup, max_generated_tokens=60, seed=6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "building-above",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Context Management Example\n",
    "Context is maintained between generate() calls automatically.\n",
    "Use 'clear_context()' to start fresh conversations\n",
    "Use 'get_context_usage_size()' to see how many tokens are used in this conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your profession?\"}\n",
    "]\n",
    "\n",
    "print(llm.generate_all(question, max_generated_tokens=30))\n",
    "new_question = [\n",
    "    {\"role\": \"user\", \"content\": \"What were we just discussing?\"}\n",
    "]\n",
    "print(llm.generate_all(new_question, max_generated_tokens=30))\n",
    "print(\"Number of tokens in the context - {}\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Clear context and ask again\n",
    "llm.clear_context()\n",
    "print(\"Number of tokens in the context after 'llm.clear_context()' - {}\".format(llm.get_context_usage_size()))\n",
    "\n",
    "print(llm.generate_all(new_question, max_generated_tokens=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187b561",
   "metadata": {},
   "source": [
    "## Advanced Context Management\n",
    "\n",
    "The LLM API provides context management capabilities for monitoring and controlling conversation context. This is essential for:\n",
    "\n",
    "- **Memory Management**: Monitor context usage to avoid exceeding `max_context_capacity`, which can affect model accuracy\n",
    "- **Conversation Persistence**: Save and restore conversation states across sessions\n",
    "- **Multi-Session Applications**: Maintain separate contexts for different users or topics\n",
    "\n",
    "**Key Functions:**\n",
    "\n",
    "- `get_context_usage_size()`: Returns current number of tokens in context\n",
    "- `max_context_capacity()`: Returns maximum context capacity of the model  \n",
    "- `clear_context()`: Resets context usage to 0 and clears conversation history\n",
    "- `save_context()`: Saves current context as binary data\n",
    "- `load_context(context_data)`: Loads a previously saved context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Context Management Example\n",
    "\n",
    "# Check initial state\n",
    "llm.clear_context()\n",
    "print(\"Max context capacity: {} tokens\".format(llm.max_context_capacity()))\n",
    "print(\"Initial context usage: {} tokens\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Build conversation context\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning briefly.\"}\n",
    "]\n",
    "response = llm.generate_all(conversation, max_generated_tokens=30)\n",
    "print(\"Response:\", response)\n",
    "print(\"Context usage after generation: {} tokens\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Save context\n",
    "saved_context = llm.save_context()\n",
    "print(\"Context saved ({} bytes)\".format(len(saved_context)))\n",
    "\n",
    "# Clear context and demonstrate it's empty\n",
    "llm.clear_context()\n",
    "print(\"After clear_context(): {} tokens\".format(llm.get_context_usage_size()))\n",
    "\n",
    "# Load saved context and continue conversation\n",
    "llm.load_context(saved_context)\n",
    "print(\"After load_context(): {} tokens\".format(llm.get_context_usage_size()))\n",
    "continuation = [{\"role\": \"user\", \"content\": \"Give me an example.\"}]\n",
    "print(\"Continuation:\", llm.generate_all(continuation, max_generated_tokens=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pink-mouse",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Generation Parameters Example\n",
    "seed - used to ensure reproducible results.\n",
    "more configurable parameters can be found in the API documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about AI.\"}\n",
    "]\n",
    "# Reproducible with seed\n",
    "llm.clear_context()\n",
    "response_seed1 = llm.generate_all(test_prompt, seed=42, temperature=0.8, max_generated_tokens=25)\n",
    "llm.clear_context()\n",
    "response_seed2 = llm.generate_all(test_prompt, seed=42, temperature=0.8, max_generated_tokens=25)\n",
    "print(\"Seed=42 (run 1): {}\".format(response_seed1))\n",
    "print(\"Seed=42 (run 2): {}\".format(response_seed2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tall-horse",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Raw Prompts vs Structured Prompts Example\n",
    "Raw prompt is a single string, usually wrapped with special tokens that are different per model.\n",
    "Here we demonstrates the tokens for QWEN family. Special tokens and prompt structures can be obtained using 'llm.prompt_template()'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.clear_context()\n",
    "raw_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat is machine learning?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "with llm.generate(raw_prompt, max_generated_tokens=30, seed=100) as generation:\n",
    "    print(\"\".join(generation))\n",
    "print()\n",
    "\n",
    "llm.clear_context()\n",
    "\n",
    "# Structured prompt (recommended)\n",
    "structured_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "]\n",
    "print(llm.generate_all(structured_prompt, max_generated_tokens=30, seed=100))\n",
    "print()\n",
    "print(llm.prompt_template())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenize-allday",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Tokenization Example\n",
    "The GenAI HEF comes with tokenization information, allowing the encoding of text into tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"Hello world\",\n",
    "    \"Machine learning with Hailo\",\n",
    "    \"The quick brown fox jumps!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = llm.tokenize(text)\n",
    "    print(\"'{}' {} tokens: {}\".format(text, len(tokens), tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stop-tomorrow",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Stop Tokens and Recovery Sequence Example\n",
    "The generation will stop when one of these conditions is met:\n",
    "\n",
    "**Max Tokens Reached**\n",
    "\n",
    "* in this case, a custom recovery-sequence will be fed into the LLM\n",
    "* most models comes with default recovery sequence, which is retrievable and configurable using 'llm.get_generation_recovery_sequence()' and 'llm.set_generation_recovery_sequence()'.\n",
    "* the recovery-sequence tokens are not counted for 'max_generated_tokens'.\n",
    "\n",
    "**Logical End of Generation**\n",
    "\n",
    "* whenever hitting one of the stop_sequences, the model finishes its generation 'gracefully' (-> without any recovery-sequence).\n",
    "* a stop-token can be any sequence of tokens (str). the generation will stop when the exact string is generated as a sequence.\n",
    "* most models comes with default stop tokens, which are retrievable and configurable using 'llm.get_stop_tokens()' and 'llm.set_stop_tokens()'.\n",
    "* all stop tokens are checked after each generated token. Setting too many can affect performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current stop tokens\n",
    "original_stop_tokens = llm.get_stop_tokens()\n",
    "print(\"Original stop tokens: {}\".format(original_stop_tokens))\n",
    "\n",
    "# Set custom stop tokens\n",
    "custom_stop_tokens = [\".\", \"END\", \"\\\\n\"]\n",
    "llm.set_stop_tokens(custom_stop_tokens)\n",
    "print(\"Custom stop tokens: {}\".format(llm.get_stop_tokens()))\n",
    "print()\n",
    "\n",
    "# Set empty stop tokens - model will stop generation only when 'max_generated_tokens' is reached\n",
    "llm.set_stop_tokens([])\n",
    "test_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Count to 5: 1, 2, 3, 4, 5. Done.\"}\n",
    "]\n",
    "print(llm.generate_all(test_prompt, max_generated_tokens=10))\n",
    "print()\n",
    "\n",
    "# Reset stop tokens\n",
    "llm.set_stop_tokens(original_stop_tokens)\n",
    "print(\"Reset stop tokens: {}\".format(llm.get_stop_tokens()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-resources",
   "metadata": {},
   "source": [
    "## Cleanup and Resource Management\n",
    "\n",
    "Properly clean up resources when done (best practice: use context managers when possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "release-and-complete",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.release()\n",
    "vdevice.release()\n",
    "print(\"Resources released successfully\")\n",
    "print(\"LLM tutorial completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
